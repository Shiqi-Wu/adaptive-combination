{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import *\n",
    "import sys \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from loss_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = diffusion_equation()\n",
    "dataset = data_model.generate_training_data(500, 10, dlt_t = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define a simple model with a single trainable parameter `k`\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.k = nn.Parameter(torch.randn(()))  # Initialize k as a parameter\n",
    "\n",
    "    def forward(self, lace_1, lace_2):\n",
    "        g = lace_1\n",
    "        h = self.k * lace_1 + lace_2\n",
    "        return g, h\n",
    "\n",
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "for batch in data_loader:\n",
    "    x, y, lace_1, lace_2 = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2])\n",
      "torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "g, h = model(lace_1, lace_2)\n",
    "x = x.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "g = g.reshape(-1, 1)\n",
    "h = h.reshape(-1, 1)\n",
    "gh = torch.cat([g, h], dim=1)\n",
    "q, _ = torch.linalg.qr(gh)\n",
    "print(gh.shape)\n",
    "print(q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2459, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(terminal_loss(q, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_1, lambda_2 = 1, 1\n",
    "def loss_function(y, g, h):\n",
    "    h_orthogonal = h / torch.linalg.norm(h, dim=1).reshape(-1, 1)\n",
    "    gh = torch.cat([g, h], dim=1)\n",
    "    q, _ = torch.linalg.qr(gh)\n",
    "    return lambda_1 * terminal_loss(q, y) + lambda_2 * orthogonal_loss(g, h_orthogonal)\n",
    "\n",
    "def train_one_epoch(model, optimizer, scheduler, train_loader, epoch):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Iterate over batches\n",
    "    for batch_idx, (_, _, lace_1, lace_2) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        _, y, lace_1, lace_2 = batch\n",
    "\n",
    "        # Compute g and h using your model. This is model-specific and might look different.\n",
    "        g, h = model(lace_1, lace_2)\n",
    "\n",
    "        # Compute the custom loss\n",
    "        loss = loss_function(y, g, h)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # if batch_idx % 100 == 0:\n",
    "            # print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training Epoch: {epoch+1}, Average Loss: {avg_loss:.4e}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 500\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 1900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 2900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 3900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 4900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 5900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 6900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 7900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 8900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 9900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 10900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 11900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 12900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 13900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 14900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 15900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 16900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 17900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 18900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 19900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 20900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 21900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 22900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 23900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 24900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 25900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 26900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27200/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27300/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27400/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27500/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27600/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27700/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27800/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 27900/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 28000/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 28100/28204, Loss: 0.5630\n",
      "Epoch 1, Batch 28200/28204, Loss: 0.5630\n",
      "Training Epoch: 1, Average Loss: 5.6299e-01\n",
      "Epoch 2, Batch 0/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 1900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 2900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 3900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 4900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 5900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 6900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 7900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 8900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 9900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 10900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 11900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 12900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13400/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13500/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13600/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13700/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13800/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 13900/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 14000/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 14100/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 14200/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 14300/28204, Loss: 0.5630\n",
      "Epoch 2, Batch 14400/28204, Loss: 0.5630\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "para_history = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, scheduler, data_loader, epoch)\n",
    "    train_loss_history.append(train_loss)\n",
    "    para_history.append(model.k.clone().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "para_history = np.array(para_history)\n",
    "print(para_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
